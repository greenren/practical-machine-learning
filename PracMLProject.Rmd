---
title: "Practical Machine Learning Project"
author: "Tim Kok"
date: "January 22, 2015"
output: html_document
---

In this project we will be predicting different types of human activity based on the measurements of wearable accelerometers. By using the random forests algorithm, we get highly accurate predictions.

## Data preparation
The data is downloaded and loaded into a training and testing dataset. 

```{r cache=TRUE}
# create data folder
if(!file.exists('data')){dir.create('data')} 

# download training and testing data
urlTrain <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
urlTest <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
download.file(urlTrain, destfile='./data/pml-training.csv', method='curl')
download.file(urlTest, destfile='./data/pml-testing.csv', method='curl')

# load datasets (ensuring both 'NA', '#DIV/0!', and empty strings are listed NA)
training <- read.csv('data/pml-training.csv', na.strings=c("","NA", "#DIV/0!"))
testing <- read.csv('data/pml-testing.csv', na.strings=c("","NA", "#DIV/0!"))
```


A number of features has a large number of missing values. These features are removed from the dataset, as well as certain features that contain information that is not helpful for prediction purposes (such as the name of the subject or the timestamp).
```{r}
# remove varaibles with too many NAs
sumNA <- function(x){sum(is.na(x))}
lowNAs <- lapply(training, FUN=sumNA) < 19000
training <- training[,lowNAs]
# remove column 1 to 7 with irrelevant information (name, timestamp, etc.)
training <- training[,-c(1:7)]
dim(training)
```

The initial training data will be divided into a training set and a testing set ('trainTrain', and 'testTrain'), to assess the out of sample error in cross validation. 
```{r message=FALSE, warning=FALSE}
# load caret and randomForest package
library(caret)
library(randomForest)
# set seed for reproducibility
set.seed(4242)
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
trainTrain <- training[inTrain,] 
testTrain <- training[-inTrain,]
dim(trainTrain)
```

## Training and Cross Validation

Since we're mainly interested in the accuracy of our predictions, we will use the random forests algorithm. Although random forests diminishes the interpretatiblity of our model, it decreases the risk of overfitting and is more efficient than boosting. We use all the features in the training set.
```{r}
modFit0 <- randomForest(classe ~ ., data=trainTrain)
modFit0
```

As the confusion matrix above shows, the out-of-bag (oob) error rate is estimated to be 0.65%, which is very low and a good sign. We expect the out of sample error to be close to this error rate, but probably a little higher.

Although the random forest algorithm already does cross-validation in the process (which results in the oob error rate estimate), it cannot hurt to estimate the out of sample error with our own cross-validation:

```{r}
pred0 <- predict(modFit0, newdata=testTrain)
confusionMatrix(pred0, testTrain$classe)
```

The confusion matrix shows that the out of sample error is also very low, about 0.78%. All in all, the model seems to give very accurate predictions.

Finally, when we apply this model to the testing set, it correctly predicts 20 out of 20 activities based on the measurements from the accelerometers.
